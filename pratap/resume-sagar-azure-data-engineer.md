# ðŸ“„ Resume â€“ Sagar | Azure Data Engineer (3+ YOE)

**Sagar Kumar**
ðŸ“ž Phone: +91-98765-43210 | âœ‰ï¸ Email: sagar.kumar@email.com
ðŸŒ LinkedIn: linkedin.com/in/sagar-kumar-azure | ðŸ“ Location: Bangalore, Karnataka

## Professional Summary

Azure Data Engineer with 3+ years of experience in building and optimizing data pipelines, ETL workflows, and cloud-based data platforms. Proficient in Azure Data Factory (ADF), Azure Databricks, Synapse Analytics, and Data Lake. Skilled in data modeling, SQL optimization, and big data processing to deliver reliable data solutions for analytics and business intelligence.

## Technical Skills

**Cloud & Data Platforms:** Azure Data Factory, Azure Databricks, Azure Synapse Analytics, Azure Data Lake (Gen2), Snowflake

**Big Data & Processing:** PySpark, Apache Spark, Delta Lake, Hadoop, Hive

**Databases & Warehousing:** SQL Server, PostgreSQL, MySQL, Azure SQL DB

**Programming & Scripting:** Python, SQL, Scala (basic)

**Data Modeling & ETL:** Star/Snowflake Schema, Slowly Changing Dimensions, Medallion Architecture

**Visualization:** Power BI, Tableau

**Version Control & CI/CD:** Git, GitHub, Jenkins, Azure DevOps

**Other Tools:** Docker, Kubernetes

## Professional Experience

### Azure Data Engineer
**[Company Name] â€“ [Location]**
ðŸ“… Jan 2022 â€“ Present

- Designed and deployed 30+ ADF pipelines to automate ingestion from on-premises SQL Server, REST APIs, and flat files into Azure Data Lake Gen2.
- Implemented Medallion Architecture (Bronze, Silver, Gold layers) in Azure Databricks using PySpark for data cleansing, transformation, and enrichment.
- Developed Delta Lake tables to enable ACID transactions and time travel for data consistency.
- Built fact and dimension models in Azure Synapse Analytics to support enterprise reporting.
- Optimized PySpark jobs improving performance by 40% for large-scale data processing (>500GB).
- Integrated CI/CD pipelines with Azure DevOps for automated deployment of ADF pipelines and Databricks notebooks.
- Partnered with Data Scientists and Analysts to provide curated datasets for ML models and BI dashboards.

### Data Engineer
**[Previous Company Name] â€“ [Location]**
ðŸ“… Jan 2020 â€“ Dec 2021

- Developed ETL pipelines using Python and SQL to integrate data from multiple relational databases into a central warehouse.
- Worked with AWS S3, Glue, and Redshift for building scalable data solutions.
- Created stored procedures, views, and optimized SQL queries for reporting needs.
- Supported Power BI dashboards by providing curated datasets.

## Education

ðŸŽ“ **Bachelor of Technology (B.Tech) â€“ Computer Science**
[University Name], [Year of Graduation]

## Projects (Azure-Focused)

### Real-Time Sales Data Platform
- **Description:** Built comprehensive real-time data processing system for retail sales analytics across multiple channels including online, mobile, and in-store transactions. The platform processes millions of daily transactions and provides instant insights for business decision-making.
- **Key Achievements:** Built real-time ingestion pipelines using ADF + Event Hub + Databricks for processing retail sales transactions.
- **Impact:** Designed Synapse Analytics models for reporting sales KPIs, resulting in 40% faster reporting and 99.9% data accuracy.

### Customer 360 Data Lake
- **Description:** Developed unified customer data platform that consolidates customer information from multiple sources including CRM, ERP, marketing automation, and loyalty programs. The system provides a single source of truth for customer data across all business functions.
- **Key Achievements:** Unified customer data from CRM, ERP, and marketing systems into Azure Data Lake.
- **Impact:** Implemented PySpark transformations in Databricks to create a 360Â° customer profile, enabling personalized marketing campaigns that increased customer engagement by 35%.
